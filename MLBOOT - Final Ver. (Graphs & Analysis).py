
# coding: utf-8

# In[38]:


# data analysis and wrangling
import pandas as pd
import numpy as np

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

#settings
pd.options.display.max_columns = 250
pd.options.display.max_rows = 250
sns.set(style="whitegrid")


# ## Column based analysis function
# <strong>Parameters</strong> <br/>
# - <strong>`df`</strong> : Data frame containing the data.
# - <strong>`col_analysed`</strong> - The feature column whose impact you want to see on the target variable
# - <strong>`target_col`</strong> - It is generally equivalent to the target variable for ex. 'UHGPrimacy' in our case.

# In[39]:


def column_based_analysis(df, col_analysed, target_col):
    df_dist = df.groupby([col_analysed, target_col]).agg({target_col: 'count'})
    df_dist.rename(columns={target_col:'record_count'}, inplace=True)
    df_dist['primacy_percent'] = df_dist['record_count'].groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))
    df_total=df.groupby([col_analysed]).agg({col_analysed:'count'})
    df_total.rename(columns={col_analysed:'total_records'}, inplace=True)
    df_total['dataset_percent'] = df_total['total_records'].apply(lambda x: (x/float(len(df.index)))*100)
    df_dist = df_dist.join(df_total).sort_values(by=['record_count'],ascending=False)
    return df_dist


# ## Function that creates graph from the output of column based analysis function
# #### Parameters
# - <strong>`df_graph`</strong> : Data frame containing the data.
# - <strong>`n`</strong> - number of values you want to analyze. Recommended range (5 -20)
# - <strong>`col_analysed`</strong> - number of the column whose values are used to create the graph.

# In[40]:


def graph_for_column_analysis(df_graph, n, col_analysed):
    # convert index into columns.
    df_graph.reset_index(inplace= True)
    
    # storing top 10 values from col_analysed column, we will generate graphs for these values only.
    top_col_analysed = list(df_graph[col_analysed][:n])
    
    # fetching all records that have values for col_analysed column present in top_col_analysed list
    df_analysed_graph = df_graph[df_graph[col_analysed].isin(top_col_analysed)]
    
    #plotting graph
    plt.show()
    plt.figure(figsize = (20, 5))
    sns_plot = sns.barplot(x = df_analysed_graph[col_analysed], y = df_analysed_graph.record_count, hue = df_analysed_graph.UHGPrimacy, data = df_analysed_graph)
    sns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation = 45, fontsize = 12)
    sns_plot.set_title('Total Records in our dataset: {}'.format(df.shape[0]), fontsize = 20)
    sns_plot


# ## Date encoding function
# #### Parameters
# - <strong>`x`</strong> : Single value in a column. `x` will be a single value of datetime data type.

# In[41]:


# function to encode date columns
def encode_date_column(x):
    if x == None:
        date = '00000000'
    else:
        year = x[0:4]
        month = x[5:7]
        day = x[8:10]
        date = year+month+day
    return(date)


# ## Loading data
# #### *Important Note* -  
# - For the demonstration purposes we are using the selected feature from our previous notebook. You can use any column you want to analyse. You can add the column name in the text file or append the column name to the list by writing below code
# > `selected_feature.append(name_of_the_new_column_i_want_to_add)`

# In[42]:


# change value of target_col according to your scenario
target_col = 'UHGPrimacy'


# In[43]:


df = pd.read_pickle("C:\Git Repos\OPIMLBoot\wowpickle.pkl")

# Picking 73,000 records randomly as the data set is very big. Dont use the following line while doing proper analysis.
df = df.sample(73000)

# Selecting only those columns that were generated by MLBOOT- Final Ver. These columns are saved in selected_feature_list.txt
# Reading features from text file into the list selected_feature

selected_feature = []
selected_feature.append(target_col)

with open('C:\Git Repos\ML\selected_feature_list.txt', 'r') as f:
    for line in f:
        selected_feature.append(line.strip())

df = df[selected_feature]

df.head()


# ### Visualizing distribution of target column i.e. target_col

# In[44]:


print(df[target_col].value_counts())
ax = sns.countplot(df[target_col],label="Count")


# ## Sorting columns into their respective category (categorical, numerical and date)

# In[45]:


# sorting into categorical and numerical list using dtype i.e. data type of the column
categorical_list = []
numerical_list = []
for i in df.columns.tolist():
    if df[i].dtype=='object':
        categorical_list.append(i)
    else:
        numerical_list.append(i)

print('Number of categorical features:', str(len(categorical_list)))


# In[46]:


date_list = []
new_list2 = []
print("No. of categorical columns before removing date columns: ", len(categorical_list))
for item in categorical_list:
    if item[-4:] == 'DATE' or item[-3:] == '_DT' or item[-3:] == 'DOB':
        date_list.append(item)
    else:
        new_list2.append(item)
    
print("No. of categorical columns after removing date columns: ", len(new_list2))
categorical_list = new_list2
date_list


# ### Processing data for correlation calculation

# In[47]:


# making new data frame for processed data
df_processed = df.copy()

# applying encoding function to columns present in date list
for item in date_list:
    df_processed[item] = df_processed[item].apply(encode_date_column)


# In[48]:


# using label encoder for encoding categorical values
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict

#dictionary for storing label encoder for each feature
le_dict = defaultdict(LabelEncoder)

for item in categorical_list:
    df_processed[item] = le_dict[item].fit_transform(df_processed[item].astype(str))

df_processed[categorical_list].sample(5)


# In[49]:


# filling null values with median value i.e. for column A we will fill null values with the median value of column A
from sklearn.preprocessing import Imputer

df1 = pd.DataFrame(Imputer(strategy='median').fit_transform(df_processed))
df1.columns = df_processed.columns
df1.index = df_processed.index
df_processed = df1.copy()


# ### Correlation 

# In[50]:


#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(df_processed.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)


# ## Important Variables

# In[51]:


# creating correlation dataframe
df_corr = df_processed.corr()

# setting threshold values for extracting important variables. You can set up your own threshold.
threshold = 0.3
important_var = [col for col in df_corr.index if df_corr.UHGPrimacy[col] > threshold]

# removing 'UHGPrimacy' from important_var list because the collinearity of 'UHGPrimacy' to 'UHGPrimacy' is 1, as shown
# in the above graph.
important_var.remove('UHGPrimacy')
important_var


# ## Single feature column analysis with respect to Target Column
# ##### When you run the function - 'column_based_analysis' - you will see a dataframe similiar to the on shown below.

# | GROUPCODE | UHGPRIMACY | record_count | primacy_percent | total_records | dataset_percent |
# | ------ | ----- | ----- | ----- | ----- | ----- | ----- |
# | 69003 | 2.0 | 6293 | 92.9 | 6771 | 1.648 |
# | 63441 | 2.0 | 6075 | 94.98 | 6396 | 1.55 |
# | 73408 | 2.0 | 5934 | 87.70 | 6766 | 1.64 |

# ##### Lets break down what you are seeing in the data frame.
# - `GROUP CODE` - This is the column that is being analysed. Each row in this column contains a groupcode eg. 69003, 63441 etc.<br/>
# - `record_count` - Example - If groupcode is 69003 then UHGPrimacy is 2.0 for 6293 records.<br/>
# - `primacy_percent` - Its value is calculated as `(record_count/total_records)*100`.<br/>
# - `total_records` - How many records are there for a specific groupcode. Example - there are 6771 records that have groupcode = 69003.<br/>
# - `dataset_percent` - Its value is calculated as `(total_records/total records in the dataset)*100`.

# In[52]:


# set the value of the column to be analysed.
column_to_be_analysed = 'GROUPCODE'


# In[53]:


df_analysed = column_based_analysis(df, col_analysed=column_to_be_analysed, target_col=target_col)
df_analysed


# In[54]:


df_analysed_copy = df_analysed.copy()
graph_for_column_analysis(df_graph = df_analysed_copy, n = 10, col_analysed = column_to_be_analysed)


# In[55]:


# set the value of the column to be analysed.
column_to_be_analysed = 'BUSINESS_ID'


# In[56]:


df_analysed = column_based_analysis(df, col_analysed=column_to_be_analysed, target_col=target_col)
df_analysed


# In[57]:


df_analysed1 = df_analysed.copy()
graph_for_column_analysis(df_graph = df_analysed1, n = 15, col_analysed = column_to_be_analysed)


# In[58]:


# set the value of the column to be analysed.
column_to_be_analysed = 'WORKINGSTATUS'


# In[59]:


df_analysed = column_based_analysis(df, col_analysed=column_to_be_analysed, target_col=target_col)
df_analysed


# In[60]:


df_analysed1 = df_analysed.copy()
graph_for_column_analysis(df_graph = df_analysed1, n = 10, col_analysed = column_to_be_analysed)

